{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats701 Homework 10, Winter 2018\n",
    "### David Guo\n",
    "#### davidguo@umich.edu\n",
    "\n",
    "Discussion: Problem 1, discussed with Jeanhee Pak. Problem 2, discussed with Jeanhee Pak, Noah Gale. Problem 3, discussed with Noah Gale. Problem 4, discussed with Jeanhee Pak and Noah Gale.\n",
    "\n",
    "Time to do each homework problem:\n",
    "\n",
    "- Problem 1 took about 15 minutes\n",
    "\n",
    "- Problem 2 took about 15 hours\n",
    "    \n",
    "- Problem 3 took about 5 hours\n",
    "\n",
    "- Problem 4 took about 20 hours\n",
    "\n",
    "- Write-up took about 3 hours\n",
    "\n",
    "### 1\n",
    "\n",
    "#### 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 1],\n",
       "        [1, 1, 1]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 1, 1],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0],\n",
       "        [0, 0, 0],\n",
       "        [0, 0, 1],\n",
       "        [0, 0, 0]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top to bottom of height, towards t shadow, towards f shadow\n",
    "np.array([[[0,0,1],[0,0,1],[0,0,1],[1,1,1]], #top slice\n",
    "          [[0,0,0],[0,0,0],[0,0,1],[0,0,0]], #2nd slice\n",
    "          [[0,0,0],[0,0,0],[0,1,1],[0,0,0]], #middle slice\n",
    "          [[0,0,0],[0,0,0],[0,0,1],[0,0,0]], #4th slice\n",
    "          [[0,0,0],[0,0,0],[0,0,1],[0,0,0]]]) #bottom slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logo = tf.constant([[[0,0,1],[0,0,1],[0,0,1],[1,1,1]], #top slice\n",
    "          [[0,0,0],[0,0,0],[0,0,1],[0,0,0]], #2nd slice\n",
    "          [[0,0,0],[0,0,0],[0,1,1],[0,0,0]], #middle slice\n",
    "          [[0,0,0],[0,0,0],[0,0,1],[0,0,0]], #4th slice\n",
    "          [[0,0,0],[0,0,0],[0,0,1],[0,0,0]]], dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Const:0' shape=(5, 4, 3) dtype=float32>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 1. 1.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 1.]\n",
      "  [0. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "with sess.as_default():\n",
    "    print(logo.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 2\n",
    "\n",
    "Solutions follow the seed 123.\n",
    "\n",
    "#### 2.1\n",
    "Use the Bernoulli likelihood function\n",
    "\n",
    "$L\\left(W,b\\right)\t=\\prod_{i=1}^{n}f\\left(y_{i};x,W,b\\right)\n",
    "\t=\\prod_{i=1}^{n}p\\left(x_{i}\\right)^{y_{i}}\\left(1-p\\left(x_{i}\\right)\\right)^{1-y_{i}}$\n",
    "    \n",
    "$\\log l\\left(W,b,x;y\\right)$\n",
    "\n",
    "$=\\log\\left[\\prod_{i=1}^{n}p\\left(x_{i}\\right)^{y_{i}}\\left(1-p\\left(x_{i}\\right)\\right)^{1-y_{i}}\\right]$\n",
    "\n",
    "$=\\sum_{i=1}^{n}\\log p\\left(x_{i}\\right)^{y_{i}}+\\sum_{i=1}^{n}\\log\\left(1-p\\left(x_{i}\\right)\\right)^{1-y_{i}}$\n",
    "\n",
    "$=\\sum_{i=1}^{n}y_{i}\\log\\left[p\\left(x_{i}\\right)\\right]+\\sum_{i=1}^{n}\\left(1-y_{i}\\right)\\log\\left(1-p\\left(x_{i}\\right)\\right)$\n",
    "\n",
    "$=\\sum_{i=1}^{n}\\log\\left(1-p\\left(x_{i}\\right)\\right)+\\sum_{i=1}^{n}y_{i}\\left(\\log\\left[p\\left(x_{i}\\right)\\right]-\\log\\left(1-p\\left(x_{i}\\right)\\right)\\right)$\n",
    "\n",
    "$=\\sum_{i=1}^{n}\\log\\left(1-p\\left(x_{i}\\right)\\right)+\\sum_{i=1}^{n}y_{i}\\left(\\frac{\\log\\left[p\\left(x_{i}\\right)\\right]}{\\log\\left(1-p\\left(x_{i}\\right)\\right)}\\right)$\n",
    "\n",
    "$=\\sum_{i=1}^{n}\\log\\left(\\frac{e^{-\\left(W^{T}x_{i}+b\\right)}}{1+e^{-\\left(W^{T}x_{i}+b\\right)}}\\right)+\\sum_{i=1}^{n}y_{i}\\log\\left(\\frac{\\frac{1}{1+e^{-\\left(W^{T}X+b\\right)}}}{\\frac{e^{-\\left(W^{T}x_{i}+b\\right)}}{1+e^{-\\left(W^{T}x_{i}+b\\right)}}}\\right)$\n",
    "\n",
    "$=\\sum_{i=1}^{n}\\log\\left(\\frac{1}{1+e^{W^{T}x_{i}+b}}\\right)+\\sum_{i=1}^{n}y_{i}\\left[\\log\\left(\\frac{1}{e^{-\\left(W^{T}x_{i}+b\\right)}}\\right)\\right]$\n",
    "\n",
    "$=-\\sum_{i=1}^{n}\\log\\left(1+e^{W^{T}x_{i}+b}\\right)+\\sum_{i=1}^{n}y_{i}\\left[\\log\\left(e^{W^{T}x_{i}+b}\\right)\\right]$\n",
    "\n",
    "$=-\\sum_{i=1}^{n}\\log\\left(1+e^{W^{T}x_{i}+b}\\right)+\\sum_{i=1}^{n}y_{i}\\left(W^{T}x_{i}+b\\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 6])\n",
    "W = tf.Variable(tf.zeros([6, 1]))\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss expression in tensorflow is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_sum(-tf.log(1 + tf.exp(tf.matmul(x,W) + b)) + y * (tf.matmul(x,W) + b), axis=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "xtrain = np.load('logistic_xtrain.npy')\n",
    "xtest = np.load('logistic_xtest.npy')\n",
    "ytrain = np.load('logistic_ytrain.npy')\n",
    "ytest = np.load('logistic_ytest.npy')\n",
    "import random\n",
    "random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 6]) # n x 6\n",
    "W = tf.Variable(tf.zeros([6,1])) # 6 x 1\n",
    "b = tf.Variable(tf.zeros([1])) # broadcasted when adding to xW\n",
    "y = tf.nn.softmax(tf.matmul(x,W) + b) # based on whatever is fed through x\n",
    "\n",
    "ytrue = tf.placeholder(tf.float32, [None,1])\n",
    "loss = tf.reduce_sum(-tf.log(1 + tf.exp(tf.matmul(x,W) + b)) +  y * (tf.matmul(x,W) + b), axis=[0])\n",
    "train_step = tf.train.GradientDescentOptimizer(0.05).minimize(loss, var_list = (W, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function randomly samples from the data sets that are input as arguments\n",
    "def next_batch(datx, daty, n):\n",
    "    indx = np.random.randint(len(datx), size=n)\n",
    "    return datx[indx], daty[indx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for _ in range(5000):\n",
    "    batch_xs, batch_ys = next_batch(xtrain, ytrain, 350)\n",
    "    sess.run(train_step, feed_dict = {x: batch_xs, ytrue: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimates of W and b are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 376.32022 ],\n",
       "       [ 304.4048  ],\n",
       "       [  18.014465],\n",
       "       [-360.78915 ],\n",
       "       [1110.5889  ],\n",
       "       [ 219.66277 ]], dtype=float32)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-87491.25], dtype=float32)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-17894284.], dtype=float32)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(loss, feed_dict = {x: xtest, y: ytest}) # compute loss for test values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss is on the order of  $ 10^7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W_true = tf.constant([1,1,2,3,5,8], dtype = tf.float32)\n",
    "b_true = tf.constant([-1], dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.654544e+09], dtype=float32)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(b - b_true).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9812487.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(tf.norm(W - W_true)).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of square error is on the order of $7\\times 10^9$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.6643564e+09], dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(tf.square(b - b_true) + tf.square(tf.norm(W - W_true))).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_xtrain = np.load('normal_xtrain.npy')\n",
    "normal_xtest = np.load('normal_xtest.npy')\n",
    "normal_ytrain = np.load('normal_ytrain.npy')\n",
    "normal_ytest = np.load('normal_ytest.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mn = tf.Variable(tf.zeros([3]))\n",
    "var = tf.Variable([1,1,1], dtype = tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "dist = tf.distributions.Normal(loc = mn, scale = var)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(dist.prob(x)))\n",
    "train_step2 = tf.train.AdagradOptimizer(0.01).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess2 = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for _ in range(5000):\n",
    "    batch_xs, batch_ys = next_batch(normal_xtrain, normal_ytrain, 350)\n",
    "    sess2.run(train_step2, feed_dict = {x: batch_xs, y: batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.95460665,  0.00838992,  1.0441424 ], dtype=float32)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimates for $\\mu_1, \\mu_2, \\mu_3$ (respectively) with the training data are approximately -0.95, 0.008, and 1.044."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.7144269, 1.000675 , 1.740305 ], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimates for variances $\\sigma_1^2, \\sigma_2^2, \\sigma_3^2$ (respectively) are approximately 0.71, 1.00, 1.74.\n",
    "\n",
    "#### 2.6\n",
    "\n",
    "The cross-entropy for the test data is about 777."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "777.18256"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess2.run(cross_entropy, feed_dict = {x: normal_xtest, y: normal_ytest})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mn_true = tf.constant([-1,0,3], dtype=tf.float32)\n",
    "var_true = tf.constant([0.5,1,1.5], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The squared errors for each mean parameter and its estimate is listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.0605561e-03, 7.0390677e-05, 3.8253791e+00], dtype=float32)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(mn_true - mn).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The squared errors for each variance parameter and its estimate is listed below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.5978885e-02, 4.5557505e-07, 5.7746466e-02], dtype=float32)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.square(var_true - var).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total sum of square error is listed below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.9312358"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(tf.square(mn_true - mn) + tf.square(var_true - var)).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess2.run(tf.one_hot(tf.argmax(dist.prob(x), axis=1), depth = 3),{x:normal_xtest})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "probs = sess2.run(tf.equal(y, tf.one_hot(tf.argmax(dist.prob(x), axis=1), depth = 3)), \n",
    "                  {x: normal_xtest, y: normal_ytest})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18933333333333335"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - probs.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification error is about 18.9%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3\n",
    "\n",
    "I followed the Linear models tutorial. All functions and classes are from the tutorial code. The only addditional code I needed was to define FLAGS and \\_SHUFFLE_BUFFER, as well as importing tempfile. Instead of nsteps, the batch_size and numepochs are changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_and_clean_file(filename, url):\n",
    "    \"\"\"Downloads data from url, and makes changes to match the CSV format.\"\"\"\n",
    "    temp_file, _ = urllib.request.urlretrieve(url)\n",
    "    with tf.gfile.Open(temp_file, 'r') as temp_eval_file:\n",
    "        with tf.gfile.Open(filename, 'w') as eval_file:\n",
    "            for line in temp_eval_file:\n",
    "                line = line.strip()\n",
    "                line = line.replace(', ', ',')\n",
    "                if not line or ',' not in line:\n",
    "                    continue\n",
    "                if line[-1] == '.':\n",
    "                    line = line[:-1]\n",
    "                line += '\\n'\n",
    "                eval_file.write(line)\n",
    "    tf.gfile.Remove(temp_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_and_clean_file('tut_train.csv','https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data')\n",
    "download_and_clean_file('tut_test.csv','https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Continuous columns\n",
    "age = tf.feature_column.numeric_column('age')\n",
    "education_num = tf.feature_column.numeric_column('education_num')\n",
    "capital_gain = tf.feature_column.numeric_column('capital_gain')\n",
    "capital_loss = tf.feature_column.numeric_column('capital_loss')\n",
    "hours_per_week = tf.feature_column.numeric_column('hours_per_week')\n",
    "\n",
    "education = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'education', [\n",
    "        'Bachelors', 'HS-grad', '11th', 'Masters', '9th', 'Some-college',\n",
    "        'Assoc-acdm', 'Assoc-voc', '7th-8th', 'Doctorate', 'Prof-school',\n",
    "        '5th-6th', '10th', '1st-4th', 'Preschool', '12th'])\n",
    "\n",
    "marital_status = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'marital_status', [\n",
    "        'Married-civ-spouse', 'Divorced', 'Married-spouse-absent',\n",
    "        'Never-married', 'Separated', 'Married-AF-spouse', 'Widowed'])\n",
    "\n",
    "relationship = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'relationship', [\n",
    "        'Husband', 'Not-in-family', 'Wife', 'Own-child', 'Unmarried',\n",
    "        'Other-relative'])\n",
    "\n",
    "workclass = tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "    'workclass', [\n",
    "        'Self-emp-not-inc', 'Private', 'State-gov', 'Federal-gov',\n",
    "        'Local-gov', '?', 'Self-emp-inc', 'Without-pay', 'Never-worked'])\n",
    "\n",
    "# To show an example of hashing:\n",
    "occupation = tf.feature_column.categorical_column_with_hash_bucket(\n",
    "    'occupation', hash_bucket_size=1000)\n",
    "\n",
    "# Transformations.\n",
    "age_buckets = tf.feature_column.bucketized_column(\n",
    "    age, boundaries=[18, 25, 30, 35, 40, 45, 50, 55, 60, 65])\n",
    "\n",
    "education_x_occupation = tf.feature_column.crossed_column(\n",
    "    ['education', 'occupation'], hash_bucket_size=1000)\n",
    "\n",
    "age_buckets_x_education_x_occupation = tf.feature_column.crossed_column(\n",
    "    [age_buckets, 'education', 'occupation'], hash_bucket_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/fq/10gmlvj140j067qh3_zsfhbh0000gn/T/tmpnu10a7el', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x10f9ce0f0>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "base_columns = [\n",
    "    education, marital_status, relationship, workclass, occupation,\n",
    "    age_buckets,\n",
    "]\n",
    "crossed_columns = [\n",
    "    tf.feature_column.crossed_column(\n",
    "        ['education', 'occupation'], hash_bucket_size=1000),\n",
    "    tf.feature_column.crossed_column(\n",
    "        [age_buckets, 'education', 'occupation'], hash_bucket_size=1000),\n",
    "]\n",
    "\n",
    "import tempfile\n",
    "\n",
    "model_dir = tempfile.mkdtemp()\n",
    "model = tf.estimator.LinearClassifier(\n",
    "    model_dir=model_dir, feature_columns=base_columns + crossed_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "_CSV_COLUMNS = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education_num',\n",
    "    'marital_status', 'occupation', 'relationship', 'race', 'gender',\n",
    "    'capital_gain', 'capital_loss', 'hours_per_week', 'native_country',\n",
    "    'income_bracket'\n",
    "]\n",
    "\n",
    "_CSV_COLUMN_DEFAULTS = [[0], [''], [0], [''], [0], [''], [''], [''], [''], [''],\n",
    "                        [0], [0], [0], [''], ['']]\n",
    "\n",
    "_SHUFFLE_BUFFER = 1500\n",
    "\n",
    "def input_fn(data_file, num_epochs, shuffle, batch_size):\n",
    "    \"\"\"Generate an input function for the Estimator.\"\"\"\n",
    "    assert tf.gfile.Exists(data_file), (\n",
    "      '%s not found. Please make sure you have either run data_download.py or '\n",
    "      'set both arguments --train_data and --test_data.' % data_file)\n",
    "\n",
    "    def parse_csv(value):\n",
    "        print('Parsing', data_file)\n",
    "        columns = tf.decode_csv(value, record_defaults=_CSV_COLUMN_DEFAULTS)\n",
    "        features = dict(zip(_CSV_COLUMNS, columns))\n",
    "        labels = features.pop('income_bracket')\n",
    "        return features, tf.equal(labels, '>50K')\n",
    "\n",
    "    # Extract lines from input files using the Dataset API.\n",
    "    dataset = tf.data.TextLineDataset(data_file)\n",
    "\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=_SHUFFLE_BUFFER)\n",
    "\n",
    "    dataset = dataset.map(parse_csv, num_parallel_calls=5)\n",
    "\n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    features, labels = iterator.get_next()\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing tut_train.csv\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/fq/10gmlvj140j067qh3_zsfhbh0000gn/T/tmpnu10a7el/model.ckpt.\n",
      "INFO:tensorflow:loss = 17.32868, step = 1\n",
      "INFO:tensorflow:global_step/sec: 134.778\n",
      "INFO:tensorflow:loss = 5.653425, step = 101 (0.744 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.286\n",
      "INFO:tensorflow:loss = 12.125192, step = 201 (0.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 214.97\n",
      "INFO:tensorflow:loss = 10.929335, step = 301 (0.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 215.769\n",
      "INFO:tensorflow:loss = 5.7738132, step = 401 (0.464 sec)\n",
      "INFO:tensorflow:global_step/sec: 288.636\n",
      "INFO:tensorflow:loss = 7.6536865, step = 501 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 275.261\n",
      "INFO:tensorflow:loss = 6.5063953, step = 601 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 216.391\n",
      "INFO:tensorflow:loss = 11.939225, step = 701 (0.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.523\n",
      "INFO:tensorflow:loss = 8.104097, step = 801 (0.392 sec)\n",
      "INFO:tensorflow:global_step/sec: 212.693\n",
      "INFO:tensorflow:loss = 10.114513, step = 901 (0.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 249.86\n",
      "INFO:tensorflow:loss = 5.609912, step = 1001 (0.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 301.826\n",
      "INFO:tensorflow:loss = 6.081351, step = 1101 (0.333 sec)\n",
      "INFO:tensorflow:global_step/sec: 242.991\n",
      "INFO:tensorflow:loss = 14.144113, step = 1201 (0.410 sec)\n",
      "INFO:tensorflow:global_step/sec: 280.528\n",
      "INFO:tensorflow:loss = 5.8519435, step = 1301 (0.358 sec)\n",
      "INFO:tensorflow:global_step/sec: 314.89\n",
      "INFO:tensorflow:loss = 10.50828, step = 1401 (0.316 sec)\n",
      "INFO:tensorflow:global_step/sec: 220.563\n",
      "INFO:tensorflow:loss = 6.5615206, step = 1501 (0.455 sec)\n",
      "INFO:tensorflow:global_step/sec: 309.481\n",
      "INFO:tensorflow:loss = 7.4945025, step = 1601 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 274.44\n",
      "INFO:tensorflow:loss = 4.4731736, step = 1701 (0.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 269.254\n",
      "INFO:tensorflow:loss = 6.99819, step = 1801 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 283.053\n",
      "INFO:tensorflow:loss = 5.5522285, step = 1901 (0.352 sec)\n",
      "INFO:tensorflow:global_step/sec: 288.838\n",
      "INFO:tensorflow:loss = 9.82065, step = 2001 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 283.405\n",
      "INFO:tensorflow:loss = 9.451604, step = 2101 (0.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 266.736\n",
      "INFO:tensorflow:loss = 8.320121, step = 2201 (0.375 sec)\n",
      "INFO:tensorflow:global_step/sec: 288.411\n",
      "INFO:tensorflow:loss = 12.337018, step = 2301 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 275.895\n",
      "INFO:tensorflow:loss = 8.577624, step = 2401 (0.362 sec)\n",
      "INFO:tensorflow:global_step/sec: 277.646\n",
      "INFO:tensorflow:loss = 10.651414, step = 2501 (0.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 276.549\n",
      "INFO:tensorflow:loss = 6.918457, step = 2601 (0.362 sec)\n",
      "INFO:tensorflow:global_step/sec: 269.881\n",
      "INFO:tensorflow:loss = 9.1040535, step = 2701 (0.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 258.102\n",
      "INFO:tensorflow:loss = 5.4753094, step = 2801 (0.387 sec)\n",
      "INFO:tensorflow:global_step/sec: 251.681\n",
      "INFO:tensorflow:loss = 9.48351, step = 2901 (0.397 sec)\n",
      "INFO:tensorflow:global_step/sec: 278.785\n",
      "INFO:tensorflow:loss = 8.062684, step = 3001 (0.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.24\n",
      "INFO:tensorflow:loss = 11.358032, step = 3101 (0.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 279.514\n",
      "INFO:tensorflow:loss = 8.272547, step = 3201 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 247.391\n",
      "INFO:tensorflow:loss = 8.891254, step = 3301 (0.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 262.456\n",
      "INFO:tensorflow:loss = 11.451558, step = 3401 (0.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 273.423\n",
      "INFO:tensorflow:loss = 10.079204, step = 3501 (0.368 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.67\n",
      "INFO:tensorflow:loss = 5.281569, step = 3601 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.33\n",
      "INFO:tensorflow:loss = 7.5711226, step = 3701 (0.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.253\n",
      "INFO:tensorflow:loss = 7.8929577, step = 3801 (0.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.776\n",
      "INFO:tensorflow:loss = 5.6448927, step = 3901 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.453\n",
      "INFO:tensorflow:loss = 11.291634, step = 4001 (0.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 281.63\n",
      "INFO:tensorflow:loss = 4.71087, step = 4101 (0.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 284.632\n",
      "INFO:tensorflow:loss = 17.130407, step = 4201 (0.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 322.486\n",
      "INFO:tensorflow:loss = 10.170517, step = 4301 (0.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 257.347\n",
      "INFO:tensorflow:loss = 11.950054, step = 4401 (0.389 sec)\n",
      "INFO:tensorflow:global_step/sec: 273.987\n",
      "INFO:tensorflow:loss = 6.151117, step = 4501 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.79\n",
      "INFO:tensorflow:loss = 5.363613, step = 4601 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 270.635\n",
      "INFO:tensorflow:loss = 7.737559, step = 4701 (0.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 246.361\n",
      "INFO:tensorflow:loss = 6.459312, step = 4801 (0.406 sec)\n",
      "INFO:tensorflow:global_step/sec: 272.654\n",
      "INFO:tensorflow:loss = 5.0445485, step = 4901 (0.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 273.538\n",
      "INFO:tensorflow:loss = 7.7957172, step = 5001 (0.366 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.899\n",
      "INFO:tensorflow:loss = 9.151392, step = 5101 (0.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 331.365\n",
      "INFO:tensorflow:loss = 6.3952613, step = 5201 (0.302 sec)\n",
      "INFO:tensorflow:global_step/sec: 307.278\n",
      "INFO:tensorflow:loss = 12.810734, step = 5301 (0.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 321.675\n",
      "INFO:tensorflow:loss = 4.996303, step = 5401 (0.313 sec)\n",
      "INFO:tensorflow:global_step/sec: 319.266\n",
      "INFO:tensorflow:loss = 8.277285, step = 5501 (0.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.557\n",
      "INFO:tensorflow:loss = 9.515584, step = 5601 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 269.163\n",
      "INFO:tensorflow:loss = 5.753815, step = 5701 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 282.555\n",
      "INFO:tensorflow:loss = 9.055108, step = 5801 (0.354 sec)\n",
      "INFO:tensorflow:global_step/sec: 319.451\n",
      "INFO:tensorflow:loss = 9.555367, step = 5901 (0.313 sec)\n",
      "INFO:tensorflow:global_step/sec: 254.206\n",
      "INFO:tensorflow:loss = 10.681859, step = 6001 (0.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 265.825\n",
      "INFO:tensorflow:loss = 8.396834, step = 6101 (0.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 283.634\n",
      "INFO:tensorflow:loss = 5.1504464, step = 6201 (0.353 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.549\n",
      "INFO:tensorflow:loss = 7.037375, step = 6301 (0.372 sec)\n",
      "INFO:tensorflow:global_step/sec: 285.925\n",
      "INFO:tensorflow:loss = 6.9621606, step = 6401 (0.350 sec)\n",
      "INFO:tensorflow:global_step/sec: 268.002\n",
      "INFO:tensorflow:loss = 5.512374, step = 6501 (0.373 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6513 into /var/folders/fq/10gmlvj140j067qh3_zsfhbh0000gn/T/tmpnu10a7el/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.9359613.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.linear.LinearClassifier at 0x126ce6f60>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(input_fn=lambda: input_fn('tut_train.csv', 5, True, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing tut_test.csv\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-23-03:37:01\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/fq/10gmlvj140j067qh3_zsfhbh0000gn/T/tmpnu10a7el/model.ckpt-6513\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-23-03:37:16\n",
      "INFO:tensorflow:Saving dict for global step 6513: accuracy = 0.83575946, accuracy_baseline = 0.76377374, auc = 0.8832832, auc_precision_recall = 0.69409615, average_loss = 0.3521632, global_step = 6513, label/mean = 0.23622628, loss = 8.801918, prediction/mean = 0.23948163\n",
      "accuracy: 0.83575946\n",
      "accuracy_baseline: 0.76377374\n",
      "auc: 0.8832832\n",
      "auc_precision_recall: 0.69409615\n",
      "average_loss: 0.3521632\n",
      "global_step: 6513\n",
      "label/mean: 0.23622628\n",
      "loss: 8.801918\n",
      "prediction/mean: 0.23948163\n"
     ]
    }
   ],
   "source": [
    "results = model.evaluate(input_fn=lambda: input_fn('tut_test.csv', 5, False, 25))\n",
    "for key in sorted(results):\n",
    "    print('%s: %s' % (key, results[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This part of tutorial printed too many things and froze the notebook so I commented it out\n",
    "\n",
    "# pred_iter = model.predict(input_fn=lambda: input_fn('tut_test.csv', 1, False, 1))\n",
    "# for pred in pred_iter:\n",
    "#     print(pred['classes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/fq/10gmlvj140j067qh3_zsfhbh0000gn/T/tmp91l7mz4_', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1267e0d68>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.LinearClassifier(\n",
    "    model_dir=model_dir, feature_columns=base_columns + crossed_columns,\n",
    "    optimizer=tf.train.FtrlOptimizer(\n",
    "        learning_rate=0.1,\n",
    "        l1_regularization_strength=1.0,\n",
    "        l2_regularization_strength=1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4\n",
    "\n",
    "#### 4.1\n",
    "\n",
    "See cloud account\n",
    "\n",
    "#### 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: b'./davidguo_normal_trained/saved_model.pb'\n"
     ]
    }
   ],
   "source": [
    "mn = tf.Variable(tf.zeros([3]))\n",
    "var = tf.Variable([1,1,1], dtype = tf.float32)\n",
    "x = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)\n",
    "\n",
    "dist = tf.distributions.Normal(loc = mn, scale = var)\n",
    "\n",
    "cross_entropy = -tf.reduce_sum(y * tf.log(dist.prob(x)))\n",
    "train_step3 = tf.train.AdagradOptimizer(0.01).minimize(cross_entropy)\n",
    "\n",
    "sess3 = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "for _ in range(5000):\n",
    "    batch_xs, batch_ys = next_batch(normal_xtrain, normal_ytrain, 350)\n",
    "    sess3.run(train_step3, feed_dict = {x: batch_xs, y: batch_ys})\n",
    "\n",
    "pred = tf.one_hot(tf.argmax(dist.prob(x), axis=1), depth = 3)\n",
    "    \n",
    "tf.saved_model.simple_save(\n",
    "    sess3,\n",
    "    \"./davidguo_normal_trained\",\n",
    "    inputs = {\"x\": x},\n",
    "    outputs = {\"pred\": pred},\n",
    "    legacy_init_op=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess3.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3\n",
    "\n",
    "I used the GUI to create a bucket\n",
    "\n",
    "#### 4.4\n",
    "I used the GUI to create a new model.\n",
    "\n",
    "To create a new version with runtime 1.6, I typed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gcloud ml-engine versions create \"davidguo_hw10\" --runtime-version \"1.6\" --model \"davidguo_stat701_hw10_normal\" --origin \"gs://davidguo-stat70\n",
    "1-hw10-normal/davidguo_normal_trained\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "into the google cloud command line. I checked with "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gcloud ml-engine versions describe \"davidguo_hw10\" --model \"davidguo_stat701_hw10_normal\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which showed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "createTime: '2018-04-23T03:04:43Z'\n",
    "deploymentUri: gs://davidguo-stat701-hw10-normal/davidguo_normal_trained\n",
    "framework: TENSORFLOW\n",
    "isDefault: true\n",
    "name: projects/endless-bounty-200202/models/davidguo_stat701_hw10_normal/versions/davidguo_hw10\n",
    "pythonVersion: '2.7'\n",
    "runtimeVersion: '1.6'\n",
    "state: READY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5\n",
    "\n",
    "The json file is included in the submission.\n",
    "\n",
    "#### 4.6\n",
    "\n",
    "The model thinks $x=4$ is in the 3rd cluster. The following is the shell input and output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "gcloud ml-engine predict --model \"davidguo_stat701_hw10_normal\" --json-instances davidguo.instance.json\n",
    "\n",
    "PRED\n",
    "[0.0, 0.0, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
